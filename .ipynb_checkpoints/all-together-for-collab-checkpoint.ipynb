{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.optimizers import SGD\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.preprocessing import StandardScaler as ss, MinMaxScaler\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image size\n",
    "SIDE = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (probably untidy) functions\n",
    "def load():\n",
    "    with open('./database_processed_first.pkl','rb') as w:\n",
    "        data = pickle.load(w)\n",
    "    return data\n",
    "\n",
    "def preprocess(data, **kwargs):\n",
    "    s = MinMaxScaler().fit(data['train'][1])\n",
    "    if False: pass\n",
    "    else:\n",
    "      result = {}\n",
    "      try: a,b,c = data['test'][0][0].shape\n",
    "      except: c=0 \n",
    "      if c>1:\n",
    "        def op_1(q):\n",
    "          q=np.mean(q,2).reshape(SIDE,SIDE,1)        \n",
    "          #return np.rint(q/255)\n",
    "          return q/255\n",
    "      elif c==1:\n",
    "        def op_1(q):\n",
    "          #return np.rint(q/255)\n",
    "          return q/255\n",
    "      else:\n",
    "        def op_1(q):\n",
    "          #return np.rint(q/255).reshape(SIDE,SIDE,1)\n",
    "          return q.reshape(SIDE,SIDE,1)/255\n",
    "\n",
    "      for x in data.keys():\n",
    "        print(data[x][1].shape)\n",
    "        if True:\n",
    "          print(f'Y pair for key {x} max,min were: \\n{np.max(data[x][1],0)}'\\\n",
    "                f', {np.min(data[x][1],0)}\\n{np.max(data[x][1],0)}, '\\\n",
    "                f'{np.min(data[x][1],0)}')\n",
    "        result[x] = (np.asarray([op_1(y) for y in data[x][0]]) , \n",
    "                     s.transform(data[x][1]))\n",
    "        if True:\n",
    "          print(f'Y pair for key {x} max,min are: \\n{np.max(result[x][1],0)}'\\\n",
    "                f', {np.min(result[x][1],0)}\\n{np.max(result[x][1],0)}, '\\\n",
    "                f'{np.min(result[x][1],0)}')\n",
    "      data['scaler'] = (s)\n",
    "      with open('./processed_database_cnn.pkl','wb') as w:\n",
    "        pickle.dump(result, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Neural Network core\n",
    "def create_and_predict(data,**kwargs):\n",
    "    \"\"\"\n",
    "    kwargs: \n",
    "        neurons=32\n",
    "        epochs=50\n",
    "        learning_rate=0.01\n",
    "        batch_size=32\n",
    "        plot=False\n",
    "    \"\"\"\n",
    "    #\n",
    "    # 1) Initialize\n",
    "\n",
    "    if True:\n",
    "      def L0(ks=8,f=1,s=1, act=None, pd='valid'): \n",
    "        return  layers.Conv2D(\n",
    "                          f, #filters\n",
    "                          (ks,ks), #kernel size\n",
    "                          strides=(s, s),\n",
    "                          activation=act,\n",
    "                          padding = pd,\n",
    "                          input_shape=(SIDE, SIDE, 1),\n",
    "                          )\n",
    "\n",
    "      def L(ks=8,f=1,s=1, act=None, pd='same'):\n",
    "        return  layers.Conv2D(\n",
    "                          f, \n",
    "                          (ks,ks), \n",
    "                          strides=(s, s),\n",
    "                          activation=act,\n",
    "                          padding=pd,\n",
    "                          )\n",
    "      def MP0(ps=6, s=2):\n",
    "        return  layers.MaxPooling2D(pool_size=(ps, ps), strides=s, \n",
    "                                )\n",
    "      def MP():\n",
    "        return  layers.MaxPooling2D(pool_size=(5, 5), strides=6, \n",
    "                                )\n",
    "\n",
    "      TIMES = 3\n",
    "      out=SIDE*SIDE\n",
    "\n",
    "      model = models.Sequential()\n",
    "\n",
    "      N = 5\n",
    "      Ndense=3\n",
    "      activ = 'tanh'\n",
    "      kernel_size = 30#5\n",
    "      q = 30\n",
    "\n",
    "      if True:\n",
    "        # CONV 1\n",
    "        model.add(L0(ks=5,f=1,s=1, act=None, pd='valid')) # dof= ks**2+1\n",
    "        # POOL 1\n",
    "        model.add(MP0(ps=5,s=2))\n",
    "        # CONV 2-N\n",
    "        for _ in range(N):\n",
    "          model.add(L0(ks=kernel_size, f=1, s=1, act=activ, pd='same')) # dof= ks**2+1\n",
    "        # POOL 2\n",
    "        model.add(MP0(ps=5,s=2))\n",
    "\n",
    "        # Flatten & Dense\n",
    "        model.add(layers.Flatten()) \n",
    "        for _ in range(Ndense):\n",
    "          model.add(Dense(\n",
    "                q,#12,\n",
    "                activation=activ))\n",
    "\n",
    "      mode = ['classifier', 'regressor'][1]\n",
    "      if mode=='classifier':  \n",
    "        model.add(Dense(\n",
    "                out,\n",
    "                activation='sigmoid'))\n",
    "\n",
    "        model.compile(\n",
    "                optimizer=SGD(learning_rate=kwargs.get('learning_rate',.001)),\n",
    "                loss='binary_crossentropy',\n",
    "                metrics='accuracy',)     \n",
    "      else:  \n",
    "        model.add(Dense(\n",
    "                2,\n",
    "                activation='linear'))\n",
    "\n",
    "        model.compile(\n",
    "                optimizer=SGD(learning_rate=kwargs.get('learning_rate',.02)),\n",
    "                loss='mean_squared_error',\n",
    "                metrics='accuracy',)     \n",
    "\n",
    "    #\n",
    "    # 2) Fit\n",
    "    print(model.summary())\n",
    "    print(f'training set is shaped: {data[\"train\"][0].shape} and the first dim is the # of samples')\n",
    "    results = model.fit(\n",
    "            *data['train'],\n",
    "            batch_size=kwargs.get('batch_size',2048),\n",
    "            epochs=kwargs.get('epochs',50),\n",
    "            verbose=1,\n",
    "            validation_data=data['val'],)\n",
    "    model.save(f'./model')\n",
    "\n",
    "    #\n",
    "    # 3) Return results\n",
    "    results = results.history \n",
    "    results['ytrue_val'] = data['val'][1]\n",
    "    results['ytrue_test'] = data['test'][1]\n",
    "    results['ypred_val'] = model.predict(data['val'][0])\n",
    "    results['ypred_test'] = model.predict(data['test'][0])\n",
    "    results['specs'] = kwargs\n",
    "    with open('results.pkl','wb') as w:\n",
    "      pickle.dump(results,w)\n",
    " \n",
    "    #\n",
    "    # 4) Maybe, plot\n",
    "    if kwargs.get('plot',False):\n",
    "        regression = True\n",
    "        case = 'val'\n",
    "        if not regression:\n",
    "          f, ax = plt.subplots(1,3)\n",
    "          fpr, tpr, treshold = roc_curve(\n",
    "                results['ytrue_'+case], results['ypred_'+case]\n",
    "                    )\n",
    "          ax[0].plot(fpr, tpr)\n",
    "        \n",
    "          weights = {0:[],1:[]}\n",
    "          for i,x in enumerate(results['ypred_'+case]):\n",
    "            weights[data[case][1][i][0]] += [x[0]]\n",
    "\n",
    "       \n",
    "          ax[1].hist(weights[0],label='0',alpha=0.5)\n",
    "          ax[1].hist(weights[1],label='1',alpha=0.5)\n",
    "          ax[1].set_xlim(0,1)\n",
    "          ax[1].legend()\n",
    "\n",
    "          ax[2].plot(results['accuracy'],c='b',label='train')\n",
    "          ax[2].plot(results['val_accuracy'],c='g')\n",
    "          ax[2].plot(results['loss'],c='b')\n",
    "          ax[2].plot(results['val_loss'],c='g',label='validation')\n",
    "          ax[2].legend()\n",
    "          ax[2].set_ylim(0,1)\n",
    "\n",
    "        else:\n",
    "          plt.plot(results['accuracy'],c='b',label='train')\n",
    "          plt.plot(results['val_accuracy'],c='g')\n",
    "          plt.plot(results['loss'],c='b')\n",
    "          plt.plot(results['val_loss'],c='g',label='validation')\n",
    "          plt.legend()\n",
    "\n",
    "        plt.show()\n",
    "        if False:\n",
    "            plt.plot(\n",
    "                *roc_curve(\n",
    "                    results['ytrue_test'], results['ypred_test']\n",
    "                        )[:-1])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The database builder, step zero before preprocessing, that makes a 250M file from the 25M database in drive\n",
    "def build_database():\n",
    "  from sklearn.model_selection import train_test_split\n",
    "  with open('./database.pkl','rb') as w:\n",
    "   data = pickle.load(w)  \n",
    "  L = len(data['angle'])\n",
    "  for x in ['angle','scaling']: data[x] = np.asarray(data[x]).reshape(-1,1)\n",
    "  X,y = data['image'], np.concatenate([data['angle'], data['scaling']],1)\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=420)\n",
    "  X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=420)\n",
    "\n",
    "  data = {'train':(X_train, y_train),\n",
    "          'val':(X_val, y_val),\n",
    "          'test':(X_test, y_test),}\n",
    "  with open('./database_processed_first.pkl','wb') as w:\n",
    "   pickle.dump(data, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does this makes sense inside an Ipython Notebook? \n",
    "if __name__=='__main__':\n",
    "   \n",
    "    mode = 'cnn'\n",
    "    switch = {1:[True, False][1],\n",
    "              2:[True, False][1],\n",
    "              3:[True, False][1],}\n",
    "\n",
    "\n",
    "    if switch[1]:\n",
    "      # build database\n",
    "      build_database()\n",
    "   \n",
    "    if switch[2]:\n",
    "      # process database \n",
    "      preprocess(load(), mode=mode.upper())\n",
    "\n",
    "    if switch[3]:\n",
    "      with open(f'./processed_database_{mode}.pkl','rb') as f:\n",
    "        dat = pickle.load(f)\n",
    "      create_and_predict(dat,\n",
    "              neurons=int(sys.argv[1]), epochs=int(sys.argv[2]),plot=True, mode=mode.upper())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
