{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "all-together-for-collab.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GastonMazzei/cnn-colab/blob/main/all-together-for-collab-vgg16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rr5lADYT0tRV"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1XYOH3-uIuj",
        "outputId": "46d49cfc-1f8d-40e8-c12a-8624188bf392",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_fZlbmTt_U1"
      },
      "source": [
        "# Imports\n",
        "import cv2\n",
        "import pickle\n",
        "import sys\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D, Flatten\n",
        "from keras.models import Sequential\n",
        "from keras.losses import binary_crossentropy\n",
        "from keras.optimizers import SGD\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.preprocessing import StandardScaler as ss, MinMaxScaler\n",
        "from PIL import Image\n",
        "from math import ceil,floor\n",
        "from random import choice\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tIpEpxPt_VC"
      },
      "source": [
        "# Image size\n",
        "SIDE = 40"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wviDNqIet_VP"
      },
      "source": [
        "\n",
        "def main(img, rows, cols, angle, scaling):\n",
        "\n",
        "  # ROTATE\n",
        "  M = cv2.getRotationMatrix2D((cols/2,rows/2),angle,1)\n",
        "  new = cv2.warpAffine(img,M,(cols,rows))\n",
        "\n",
        "  # RESCALE by s\n",
        "  # shrinking uses  cv2.INTER_AREA \n",
        "  # zooming uses cv2.INTER_CUBIC (slow) OR cv2.INTER_LINEAR\n",
        "  #\n",
        "  slow = False\n",
        "  if slow: zoom_inter = cv2.INTER_CUBIC\n",
        "  else: zoom_inter = cv2.INTER_LINEAR\n",
        "  if scaling<1:\n",
        "    new = cv2.resize(new,(0,0),fx=scaling, fy=scaling, interpolation = cv2.INTER_AREA)\n",
        "    temp_rows, temp_cols = new.shape\n",
        "    if (rows-temp_rows)%2==0:\n",
        "      top,bottom = [(rows-temp_rows)//2]*2\n",
        "    else:\n",
        "      top,bottom = floor((rows-temp_rows)/2), ceil((rows-temp_rows)/2) \n",
        "    if (cols-temp_cols)%2==0:\n",
        "      left,right = [(cols-temp_cols)//2]*2\n",
        "    else:\n",
        "      left,right = floor((cols-temp_cols)/2), ceil((cols-temp_cols)/2) \n",
        "    new = cv2.copyMakeBorder(new,top, bottom, left, right,  cv2.BORDER_CONSTANT, value=0)\n",
        "  elif scaling>1:\n",
        "    new = cv2.resize(new,(0,0),fx=scaling, fy=scaling, interpolation = zoom_inter)\n",
        "    temp_rows, temp_cols = new.shape\n",
        "    new = new[(temp_rows-rows)//2:(temp_rows+rows)//2, (temp_cols-cols)//2:(temp_cols+cols)//2]\n",
        "    new = cv2.resize(new,(rows,cols), interpolation = cv2.INTER_AREA)\n",
        "\n",
        "  return new\n",
        "\n",
        "# preprocessing functions\n",
        "def preprocess(data, **kwargs):\n",
        "    s = MinMaxScaler().fit(data['train'][1])\n",
        "    if False: pass\n",
        "    else:\n",
        "      result = {}\n",
        "      try: a,b,c = data['test'][0][0].shape\n",
        "      except: c=0 \n",
        "      if c>1:\n",
        "        def op_1(q):\n",
        "          q=np.mean(q,2).reshape(SIDE,SIDE,1)        \n",
        "          #return np.rint(q/255)\n",
        "          return q/255\n",
        "      elif c==1:\n",
        "        def op_1(q):\n",
        "          #return np.rint(q/255)\n",
        "          return q/255\n",
        "      else:\n",
        "        def op_1(q):\n",
        "          #return np.rint(q/255).reshape(SIDE,SIDE,1)\n",
        "          return q.reshape(SIDE,SIDE,1)/255\n",
        "\n",
        "      for x in data.keys():\n",
        "        print(data[x][1].shape)\n",
        "        if True:\n",
        "          print(f'Y pair for key {x} max,min were: \\n{np.max(data[x][1],0)}'\\\n",
        "                f', {np.min(data[x][1],0)}\\n{np.max(data[x][1],0)}, '\\\n",
        "                f'{np.min(data[x][1],0)}')\n",
        "        if c==1: \n",
        "          print('c==1 has a speed improvement!')\n",
        "          L,*_ = data[x][0].shape\n",
        "          result[x] = (data[x][0].reshape(L,SIDE,SIDE,1)/255, s.transform(data[x][1]))\n",
        "        else:\n",
        "          result[x] = (np.asarray([op_1(y) for y in data[x][0]]) , \n",
        "                       s.transform(data[x][1]))\n",
        "        if True:\n",
        "          print(f'Y pair for key {x} max,min are: \\n{np.max(result[x][1],0)}'\\\n",
        "                f', {np.min(result[x][1],0)}\\n{np.max(result[x][1],0)}, '\\\n",
        "                f'{np.min(result[x][1],0)}')\n",
        "      data['scaler'] = (s)\n",
        "\n",
        "      small = {'test':(result['test'][0][:150], result['test'][1][:150])}\n",
        "      with open('/content/drive/My Drive/ml-output/escher/small-test.pkl','wb') as f: pickle.dump(small,f)\n",
        "      if sys.getsizeof(result)<2.5e9: \n",
        "        print('SAVING THE DATABASE!')\n",
        "        with open('/content/drive/My Drive/ml-output/escher/whole-database.pkl','wb') as f: pickle.dump(result,f)      \n",
        "      else: print('database size was (in bytes)', sys.getsizeof(result))\n",
        "\n",
        "      return result\n",
        "\n",
        "\n",
        "def first_one(N=30000):\n",
        "  firstdir = os.getcwd()\n",
        "  os.chdir('/content/drive/My Drive/nn-images')\n",
        "  images = {name:cv2.imread(name,0) for name in os.listdir() if name[-3:] in ['png','jpg']}\n",
        "  images_names = list(images.keys())\n",
        "  os.chdir(firstdir)\n",
        "  print(images_names)\n",
        "  data = {'angle':[], 'scaling':[],\n",
        "          'name':[],  'image':[],'source':[]}\n",
        "  \n",
        "  length = int((N/len(images_names))**(1/2))\n",
        "  angles = np.linspace(-90,90,length)\n",
        "  scalings = [round(2**x,3) for x in np.linspace(-1,1,length)]\n",
        "  combinations = []\n",
        "  lap = 0\n",
        "  for _1 in images_names:\n",
        "    for _2 in range(length):\n",
        "      for _3 in range(length):\n",
        "        combinations.append((angles[_2], scalings[_3], _1))\n",
        "  for x in combinations:\n",
        "    lap+=1\n",
        "    if lap%10000==0: print('lap number is ',lap)\n",
        "\n",
        "    angle = x[0]\n",
        "    scaling = x[1]\n",
        "    img_name = x[2]\n",
        "\n",
        "    img = images[img_name]\n",
        "    rows,cols = img.shape\n",
        "    result = main(img, rows, cols, angle, scaling)\n",
        "    data['angle'].append(angle)\n",
        "    data['source'].append(img_name)\n",
        "    data['scaling'].append(scaling)\n",
        "    data['image'].append(result)\n",
        "    name_index=0\n",
        "    name = f'{angle}_{scaling}_{name_index}'\n",
        "    while name in data['name']:\n",
        "      name_index+=1\n",
        "      name = f'{angle}_{scaling}_{name_index}'\n",
        "    data['name'].append(name)\n",
        "  return data\n",
        "    "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhLP8g1kt_Va"
      },
      "source": [
        "# The Neural Network core\n",
        "def create_and_predict(data,**kwargs):\n",
        "    \"\"\"\n",
        "    kwargs: \n",
        "        neurons=32\n",
        "        epochs=50\n",
        "        learning_rate=0.01\n",
        "        batch_size=32\n",
        "        plot=False\n",
        "    \"\"\"\n",
        "    print('about to build network')\n",
        "    #\n",
        "    # 1) Initialize\n",
        "\n",
        "    if False:\n",
        "      def L0(ks=8,f=1,s=1, act=None, pd='valid'): \n",
        "        return  layers.Conv2D(\n",
        "                          f, #filters\n",
        "                          (ks,ks), #kernel size\n",
        "                          strides=(s, s),\n",
        "                          activation=act,\n",
        "                          padding = pd,\n",
        "                          input_shape=(SIDE, SIDE, 1),\n",
        "                          )\n",
        "\n",
        "      def L(ks=8,f=1,s=1, act=None, pd='same'):\n",
        "        return  layers.Conv2D(\n",
        "                          f, \n",
        "                          (ks,ks), \n",
        "                          strides=(s, s),\n",
        "                          activation=act,\n",
        "                          padding=pd,\n",
        "                          )\n",
        "      def MP0(ps=6, s=2):\n",
        "        return  layers.MaxPooling2D(pool_size=(ps, ps), strides=s, \n",
        "                                )\n",
        "      def MP():\n",
        "        return  layers.MaxPooling2D(pool_size=(5, 5), strides=6, \n",
        "                                )\n",
        "\n",
        "      TIMES = 3\n",
        "      out=SIDE*SIDE\n",
        "      model = models.Sequential()\n",
        "\n",
        "      N = 6\n",
        "      Ndense=3\n",
        "      activ = 'relu'\n",
        "      kernel_size = 12#5\n",
        "      q = 40\n",
        "      if True:\n",
        "        # CONV 1\n",
        "        model.add(L0(ks=5,f=1,s=1, act=None, pd='valid')) # dof= ks**2+1\n",
        "        # POOL 1\n",
        "        model.add(MP0(ps=5,s=2))\n",
        "        # CONV 2-N\n",
        "        for _ in range(N):\n",
        "          model.add(L0(ks=kernel_size, f=1, s=1, act=activ, pd='same')) # dof= ks**2+1\n",
        "        # POOL 2\n",
        "        model.add(MP0(ps=5,s=2))\n",
        "\n",
        "        # Flatten & Dense\n",
        "        model.add(layers.Flatten()) \n",
        "        for _ in range(Ndense):\n",
        "          model.add(Dense(\n",
        "                q,#12,\n",
        "                activation=activ))\n",
        "\n",
        "    if False:\n",
        "      # LeCunn\n",
        "      model = models.Sequential()\n",
        "\n",
        "      model.add(layers.Conv2D(filters=6, kernel_size=(3, 3), activation='relu', input_shape=(40,40,1)))\n",
        "      model.add(layers.AveragePooling2D())\n",
        "\n",
        "      model.add(layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu'))\n",
        "      model.add(layers.AveragePooling2D())\n",
        "\n",
        "      model.add(layers.Flatten())\n",
        "\n",
        "      model.add(layers.Dense(units=40, activation='relu'))\n",
        "\n",
        "      model.add(layers.Dense(units=20, activation='relu'))\n",
        "\n",
        "      #model.add(layers.Dense(units=10, activation = 'softmax'))\n",
        "\n",
        "    if True:\n",
        "      # VGG16\n",
        "      input_shape = (40,40,1)#(224,224,3)\n",
        "      dense_neurons = 100 #4096=64^2\n",
        "      f_s,f_m,f_l,f_xl = 4,8,16,32 #10, 20,40, 80 #64, 128,256, 512\n",
        "      ks_big = (3,3) #(3,3)\n",
        "      ks_small = (2,2)\n",
        "      model = Sequential()\n",
        "\n",
        "      model.add(Conv2D(input_shape=input_shape,\n",
        "                      filters=f_s,kernel_size=ks_big,padding=\"same\", activation=\"relu\"))\n",
        "\n",
        "      model.add(Conv2D(filters=f_s,kernel_size=ks_big,padding=\"same\", activation=\"relu\"))\n",
        "\n",
        "      model.add(MaxPool2D(pool_size=(2,2),strides=ks_small))\n",
        "\n",
        "      #model.add(Conv2D(filters=f_m, kernel_size=ks_big, padding=\"same\", activation=\"relu\")) #mutted 1 of 3 convs\n",
        "\n",
        "      model.add(Conv2D(filters=f_m, kernel_size=ks_big, padding=\"same\", activation=\"relu\"))\n",
        "\n",
        "      model.add(MaxPool2D(pool_size=(2,2),strides=ks_small))\n",
        "\n",
        "      #model.add(Conv2D(filters=f_l, kernel_size=ks_big, padding=\"same\", activation=\"relu\"))#mutted 1 of 3 convs\n",
        "\n",
        "      model.add(Conv2D(filters=f_l, kernel_size=ks_big, padding=\"same\", activation=\"relu\"))\n",
        "\n",
        "      model.add(Conv2D(filters=f_l, kernel_size=ks_big, padding=\"same\", activation=\"relu\"))\n",
        "\n",
        "      model.add(MaxPool2D(pool_size=(2,2),strides=ks_small))\n",
        "\n",
        "      #model.add(Conv2D(filters=f_xl, kernel_size=ks_big, padding=\"same\", activation=\"relu\"))#mutted 1 of 3 convs\n",
        "\n",
        "      model.add(Conv2D(filters=f_xl, kernel_size=ks_big, padding=\"same\", activation=\"relu\"))\n",
        "\n",
        "      model.add(Conv2D(filters=f_xl, kernel_size=ks_big, padding=\"same\", activation=\"relu\"))\n",
        "\n",
        "      model.add(MaxPool2D(pool_size=(2,2),strides=ks_small))\n",
        "\n",
        "      #model.add(Conv2D(filters=f_xl, kernel_size=ks_big, padding=\"same\", activation=\"relu\"))#mutted last step because it was already 2!\n",
        "\n",
        "      #model.add(Conv2D(filters=f_xl, kernel_size=ks_big, padding=\"same\", activation=\"relu\"))\n",
        "\n",
        "      #model.add(Conv2D(filters=f_xl, kernel_size=ks_big, padding=\"same\", activation=\"relu\"))\n",
        "\n",
        "      #model.add(MaxPool2D(pool_size=(2,2),strides=ks_small))\n",
        "\n",
        "      model.add(Flatten())\n",
        "      model.add(Dense(units=dense_neurons,activation=\"relu\"))\n",
        "      model.add(Dense(units=dense_neurons,activation=\"relu\"))\n",
        "\n",
        "\n",
        "    mode = ['classifier', 'regressor'][1]\n",
        "    if mode=='classifier':  \n",
        "      model.add(Dense(\n",
        "              out,\n",
        "              activation='sigmoid'))\n",
        "\n",
        "      model.compile(\n",
        "              optimizer=SGD(learning_rate=kwargs.get('learning_rate',.001)),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics='accuracy',)     \n",
        "    else:  \n",
        "      model.add(Dense(\n",
        "              2,\n",
        "              activation='linear'))\n",
        "\n",
        "      model.compile(\n",
        "              optimizer=SGD(learning_rate=kwargs.get('learning_rate',.02)),\n",
        "              loss='mean_squared_error',\n",
        "              metrics='accuracy',)     \n",
        "\n",
        "    #\n",
        "    # 2) Fit\n",
        "    print(model.summary())\n",
        "    print(f'training set is shaped: {data[\"train\"][0].shape} and the first dim is the # of samples')\n",
        "    print('about to train')\n",
        "    results = model.fit(\n",
        "            *data['train'],\n",
        "            batch_size=kwargs.get('batch_size',256),\n",
        "            epochs=kwargs.get('epochs',50),\n",
        "            verbose=1,\n",
        "            validation_data=data['val'],)\n",
        "    model.save(f'/content/drive/My Drive/ml-output/escher/model')\n",
        "\n",
        "    #\n",
        "    # 3) Return results\n",
        "    results = results.history \n",
        "    results['ytrue_val'] = data['val'][1]\n",
        "    results['ytrue_test'] = data['test'][1]\n",
        "    results['ypred_val'] = model.predict(data['val'][0])\n",
        "    results['ypred_test'] = model.predict(data['test'][0])\n",
        "    results['specs'] = kwargs\n",
        "    important_results = {'ytrue_test':results['ytrue_test'],\n",
        "                         'ypred_test':results['ypred_test']}\n",
        "    with open('/content/drive/My Drive/ml-output/escher/results.pkl','wb') as w:\n",
        "      pickle.dump(important_results,w)\n",
        " \n",
        "    #\n",
        "    # 4) Maybe, plot\n",
        "    if kwargs.get('plot',False):\n",
        "        regression = True\n",
        "        case = 'val'\n",
        "        if not regression:\n",
        "          f, ax = plt.subplots(1,3)\n",
        "          fpr, tpr, treshold = roc_curve(\n",
        "                results['ytrue_'+case], results['ypred_'+case]\n",
        "                    )\n",
        "          ax[0].plot(fpr, tpr)\n",
        "        \n",
        "          weights = {0:[],1:[]}\n",
        "          for i,x in enumerate(results['ypred_'+case]):\n",
        "            weights[data[case][1][i][0]] += [x[0]]\n",
        "\n",
        "       \n",
        "          ax[1].hist(weights[0],label='0',alpha=0.5)\n",
        "          ax[1].hist(weights[1],label='1',alpha=0.5)\n",
        "          ax[1].set_xlim(0,1)\n",
        "          ax[1].legend()\n",
        "\n",
        "          ax[2].plot(results['accuracy'],c='b',label='train')\n",
        "          ax[2].plot(results['val_accuracy'],c='g')\n",
        "          ax[2].plot(results['loss'],c='b')\n",
        "          ax[2].plot(results['val_loss'],c='g',label='validation')\n",
        "          ax[2].legend()\n",
        "          ax[2].set_ylim(0,1)\n",
        "\n",
        "        else:\n",
        "          plt.plot(results['accuracy'],c='b',label='train')\n",
        "          plt.plot(results['val_accuracy'],c='g')\n",
        "          plt.plot(results['loss'],c='b')\n",
        "          plt.plot(results['val_loss'],c='g',label='validation')\n",
        "          plt.legend()\n",
        "\n",
        "        plt.savefig('/content/drive/My Drive/ml-output/escher/training_curve.png')\n",
        "        if False:\n",
        "            plt.plot(\n",
        "                *roc_curve(\n",
        "                    results['ytrue_test'], results['ypred_test']\n",
        "                        )[:-1])\n",
        "    return results"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGMAxL7tt_Vi"
      },
      "source": [
        "# The database builder, step zero before preprocessing, that makes a 250M file from the 25M database in drive\n",
        "def build_database(data):\n",
        "  from sklearn.model_selection import train_test_split\n",
        "  L = len(data['angle'])\n",
        "  for x in ['angle','scaling']: data[x] = np.asarray(data[x]).reshape(-1,1)\n",
        "  X,y = data['image'], np.concatenate([data['angle'], data['scaling']],1)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=420)\n",
        "  X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=420)\n",
        "\n",
        "  data = {'train':(X_train, y_train),\n",
        "          'val':(X_val, y_val),\n",
        "          'test':(X_test, y_test),}\n",
        "  return data"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9AF5BMst_Vq",
        "outputId": "adfcfb0b-9dd9-49db-e5ee-73c3cb872b11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if True:\n",
        "   \n",
        "    mode = 'cnn'\n",
        "    switch = {1:[True, False][1],\n",
        "              2:[True, False][1],\n",
        "              3:[True, False][0],}\n",
        "\n",
        "\n",
        "    if switch[3]:\n",
        "      from keras.backend import clear_session\n",
        "      clear_session()\n",
        "      print('about to preprocess')\n",
        "      #dat = preprocess(build_database(first_one(140000)), mode=mode.upper())\n",
        "      with open('/content/drive/My Drive/ml-output/escher/whole-database.pkl','rb') as f: dat=pickle.load(f)      \n",
        "      print('ended preprocessing!')\n",
        "      create_and_predict(dat,\n",
        "              epochs=250,plot=True,learning_rate=0.01, mode=mode.upper())\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "about to preprocess\n",
            "ended preprocessing!\n",
            "about to build network\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 40, 40, 4)         40        \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 40, 40, 4)         148       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 20, 20, 4)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 20, 20, 8)         296       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 10, 10, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 10, 10, 16)        1168      \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 10, 10, 16)        2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 5, 5, 32)          4640      \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 5, 5, 32)          9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 2, 2, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 100)               12900     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 202       \n",
            "=================================================================\n",
            "Total params: 41,062\n",
            "Trainable params: 41,062\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "training set is shaped: (108160, 40, 40, 1) and the first dim is the # of samples\n",
            "about to train\n",
            "Epoch 1/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0876 - accuracy: 0.6284 - val_loss: 0.0614 - val_accuracy: 0.6946\n",
            "Epoch 2/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0574 - accuracy: 0.7091 - val_loss: 0.0545 - val_accuracy: 0.7232\n",
            "Epoch 3/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0526 - accuracy: 0.7289 - val_loss: 0.0502 - val_accuracy: 0.7389\n",
            "Epoch 4/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0495 - accuracy: 0.7389 - val_loss: 0.0476 - val_accuracy: 0.7463\n",
            "Epoch 5/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0480 - accuracy: 0.7477 - val_loss: 0.0466 - val_accuracy: 0.7463\n",
            "Epoch 6/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0467 - accuracy: 0.7503 - val_loss: 0.0451 - val_accuracy: 0.7516\n",
            "Epoch 7/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0455 - accuracy: 0.7532 - val_loss: 0.0433 - val_accuracy: 0.7552\n",
            "Epoch 8/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0441 - accuracy: 0.7557 - val_loss: 0.0424 - val_accuracy: 0.7656\n",
            "Epoch 9/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0432 - accuracy: 0.7605 - val_loss: 0.0414 - val_accuracy: 0.7665\n",
            "Epoch 10/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0420 - accuracy: 0.7626 - val_loss: 0.0407 - val_accuracy: 0.7649\n",
            "Epoch 11/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0409 - accuracy: 0.7649 - val_loss: 0.0387 - val_accuracy: 0.7738\n",
            "Epoch 12/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0398 - accuracy: 0.7703 - val_loss: 0.0395 - val_accuracy: 0.7740\n",
            "Epoch 13/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0389 - accuracy: 0.7733 - val_loss: 0.0366 - val_accuracy: 0.7845\n",
            "Epoch 14/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0379 - accuracy: 0.7775 - val_loss: 0.0386 - val_accuracy: 0.7837\n",
            "Epoch 15/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0369 - accuracy: 0.7803 - val_loss: 0.0369 - val_accuracy: 0.7718\n",
            "Epoch 16/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0361 - accuracy: 0.7817 - val_loss: 0.0393 - val_accuracy: 0.7779\n",
            "Epoch 17/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0354 - accuracy: 0.7847 - val_loss: 0.0357 - val_accuracy: 0.7916\n",
            "Epoch 18/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0346 - accuracy: 0.7875 - val_loss: 0.0334 - val_accuracy: 0.8009\n",
            "Epoch 19/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0336 - accuracy: 0.7913 - val_loss: 0.0311 - val_accuracy: 0.7990\n",
            "Epoch 20/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0328 - accuracy: 0.7925 - val_loss: 0.0340 - val_accuracy: 0.7989\n",
            "Epoch 21/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0320 - accuracy: 0.7966 - val_loss: 0.0291 - val_accuracy: 0.8088\n",
            "Epoch 22/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0315 - accuracy: 0.7973 - val_loss: 0.0312 - val_accuracy: 0.7935\n",
            "Epoch 23/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0306 - accuracy: 0.8012 - val_loss: 0.0290 - val_accuracy: 0.8041\n",
            "Epoch 24/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0297 - accuracy: 0.8043 - val_loss: 0.0277 - val_accuracy: 0.8124\n",
            "Epoch 25/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0290 - accuracy: 0.8075 - val_loss: 0.0265 - val_accuracy: 0.8161\n",
            "Epoch 26/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0285 - accuracy: 0.8100 - val_loss: 0.0263 - val_accuracy: 0.8272\n",
            "Epoch 27/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0278 - accuracy: 0.8120 - val_loss: 0.0263 - val_accuracy: 0.8206\n",
            "Epoch 28/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0270 - accuracy: 0.8163 - val_loss: 0.0261 - val_accuracy: 0.8240\n",
            "Epoch 29/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0265 - accuracy: 0.8179 - val_loss: 0.0253 - val_accuracy: 0.8287\n",
            "Epoch 30/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0256 - accuracy: 0.8212 - val_loss: 0.0244 - val_accuracy: 0.8410\n",
            "Epoch 31/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0251 - accuracy: 0.8228 - val_loss: 0.0487 - val_accuracy: 0.7680\n",
            "Epoch 32/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0249 - accuracy: 0.8246 - val_loss: 0.0252 - val_accuracy: 0.8231\n",
            "Epoch 33/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0239 - accuracy: 0.8272 - val_loss: 0.0234 - val_accuracy: 0.8312\n",
            "Epoch 34/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0235 - accuracy: 0.8298 - val_loss: 0.0306 - val_accuracy: 0.7919\n",
            "Epoch 35/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0229 - accuracy: 0.8314 - val_loss: 0.0233 - val_accuracy: 0.8284\n",
            "Epoch 36/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0224 - accuracy: 0.8341 - val_loss: 0.0206 - val_accuracy: 0.8524\n",
            "Epoch 37/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0220 - accuracy: 0.8365 - val_loss: 0.0203 - val_accuracy: 0.8557\n",
            "Epoch 38/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0214 - accuracy: 0.8408 - val_loss: 0.0268 - val_accuracy: 0.8310\n",
            "Epoch 39/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0208 - accuracy: 0.8430 - val_loss: 0.0190 - val_accuracy: 0.8519\n",
            "Epoch 40/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0207 - accuracy: 0.8429 - val_loss: 0.0184 - val_accuracy: 0.8578\n",
            "Epoch 41/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0200 - accuracy: 0.8488 - val_loss: 0.0185 - val_accuracy: 0.8637\n",
            "Epoch 42/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0199 - accuracy: 0.8467 - val_loss: 0.0195 - val_accuracy: 0.8561\n",
            "Epoch 43/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0192 - accuracy: 0.8511 - val_loss: 0.0241 - val_accuracy: 0.8178\n",
            "Epoch 44/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0190 - accuracy: 0.8540 - val_loss: 0.0276 - val_accuracy: 0.8064\n",
            "Epoch 45/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0188 - accuracy: 0.8554 - val_loss: 0.0167 - val_accuracy: 0.8683\n",
            "Epoch 46/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0182 - accuracy: 0.8576 - val_loss: 0.0158 - val_accuracy: 0.8750\n",
            "Epoch 47/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0183 - accuracy: 0.8576 - val_loss: 0.0177 - val_accuracy: 0.8751\n",
            "Epoch 48/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0176 - accuracy: 0.8599 - val_loss: 0.0194 - val_accuracy: 0.8641\n",
            "Epoch 49/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0173 - accuracy: 0.8629 - val_loss: 0.0187 - val_accuracy: 0.8726\n",
            "Epoch 50/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0168 - accuracy: 0.8651 - val_loss: 0.0194 - val_accuracy: 0.8476\n",
            "Epoch 51/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0167 - accuracy: 0.8648 - val_loss: 0.0201 - val_accuracy: 0.8602\n",
            "Epoch 52/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0160 - accuracy: 0.8680 - val_loss: 0.0155 - val_accuracy: 0.8701\n",
            "Epoch 53/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0161 - accuracy: 0.8696 - val_loss: 0.0173 - val_accuracy: 0.8649\n",
            "Epoch 54/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0160 - accuracy: 0.8703 - val_loss: 0.0162 - val_accuracy: 0.8770\n",
            "Epoch 55/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0155 - accuracy: 0.8717 - val_loss: 0.0158 - val_accuracy: 0.8655\n",
            "Epoch 56/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0152 - accuracy: 0.8732 - val_loss: 0.0202 - val_accuracy: 0.8485\n",
            "Epoch 57/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0150 - accuracy: 0.8758 - val_loss: 0.0137 - val_accuracy: 0.8845\n",
            "Epoch 58/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0148 - accuracy: 0.8760 - val_loss: 0.0149 - val_accuracy: 0.8783\n",
            "Epoch 59/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0145 - accuracy: 0.8771 - val_loss: 0.0195 - val_accuracy: 0.8734\n",
            "Epoch 60/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0143 - accuracy: 0.8777 - val_loss: 0.0128 - val_accuracy: 0.8889\n",
            "Epoch 61/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0142 - accuracy: 0.8787 - val_loss: 0.0156 - val_accuracy: 0.8790\n",
            "Epoch 62/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0139 - accuracy: 0.8806 - val_loss: 0.0134 - val_accuracy: 0.8793\n",
            "Epoch 63/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0136 - accuracy: 0.8810 - val_loss: 0.0125 - val_accuracy: 0.8900\n",
            "Epoch 64/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0134 - accuracy: 0.8815 - val_loss: 0.0144 - val_accuracy: 0.8884\n",
            "Epoch 65/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0132 - accuracy: 0.8835 - val_loss: 0.0121 - val_accuracy: 0.8937\n",
            "Epoch 66/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0134 - accuracy: 0.8820 - val_loss: 0.0128 - val_accuracy: 0.8897\n",
            "Epoch 67/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0129 - accuracy: 0.8856 - val_loss: 0.0117 - val_accuracy: 0.8964\n",
            "Epoch 68/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0127 - accuracy: 0.8855 - val_loss: 0.0116 - val_accuracy: 0.8959\n",
            "Epoch 69/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0125 - accuracy: 0.8873 - val_loss: 0.0114 - val_accuracy: 0.8966\n",
            "Epoch 70/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0124 - accuracy: 0.8886 - val_loss: 0.0127 - val_accuracy: 0.8887\n",
            "Epoch 71/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0123 - accuracy: 0.8888 - val_loss: 0.0116 - val_accuracy: 0.8997\n",
            "Epoch 72/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0118 - accuracy: 0.8924 - val_loss: 0.0111 - val_accuracy: 0.8974\n",
            "Epoch 73/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0122 - accuracy: 0.8881 - val_loss: 0.0128 - val_accuracy: 0.8876\n",
            "Epoch 74/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0119 - accuracy: 0.8914 - val_loss: 0.0133 - val_accuracy: 0.8908\n",
            "Epoch 75/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0116 - accuracy: 0.8924 - val_loss: 0.0123 - val_accuracy: 0.8905\n",
            "Epoch 76/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0117 - accuracy: 0.8916 - val_loss: 0.0106 - val_accuracy: 0.9015\n",
            "Epoch 77/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0113 - accuracy: 0.8940 - val_loss: 0.0106 - val_accuracy: 0.8981\n",
            "Epoch 78/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0114 - accuracy: 0.8931 - val_loss: 0.0109 - val_accuracy: 0.8967\n",
            "Epoch 79/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0113 - accuracy: 0.8930 - val_loss: 0.0120 - val_accuracy: 0.8966\n",
            "Epoch 80/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0111 - accuracy: 0.8940 - val_loss: 0.0113 - val_accuracy: 0.8892\n",
            "Epoch 81/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0109 - accuracy: 0.8958 - val_loss: 0.0106 - val_accuracy: 0.9030\n",
            "Epoch 82/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0109 - accuracy: 0.8965 - val_loss: 0.0105 - val_accuracy: 0.9047\n",
            "Epoch 83/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0108 - accuracy: 0.8964 - val_loss: 0.0108 - val_accuracy: 0.9032\n",
            "Epoch 84/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0106 - accuracy: 0.8971 - val_loss: 0.0110 - val_accuracy: 0.9023\n",
            "Epoch 85/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0104 - accuracy: 0.8985 - val_loss: 0.0096 - val_accuracy: 0.9054\n",
            "Epoch 86/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0105 - accuracy: 0.8985 - val_loss: 0.0166 - val_accuracy: 0.8750\n",
            "Epoch 87/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0103 - accuracy: 0.8989 - val_loss: 0.0096 - val_accuracy: 0.9062\n",
            "Epoch 88/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0102 - accuracy: 0.9000 - val_loss: 0.0112 - val_accuracy: 0.8980\n",
            "Epoch 89/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0101 - accuracy: 0.9005 - val_loss: 0.0101 - val_accuracy: 0.8977\n",
            "Epoch 90/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0102 - accuracy: 0.8991 - val_loss: 0.0111 - val_accuracy: 0.8897\n",
            "Epoch 91/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0101 - accuracy: 0.9013 - val_loss: 0.0111 - val_accuracy: 0.8947\n",
            "Epoch 92/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0098 - accuracy: 0.9031 - val_loss: 0.0100 - val_accuracy: 0.8977\n",
            "Epoch 93/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0098 - accuracy: 0.9031 - val_loss: 0.0098 - val_accuracy: 0.9044\n",
            "Epoch 94/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0097 - accuracy: 0.9031 - val_loss: 0.0122 - val_accuracy: 0.8999\n",
            "Epoch 95/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0096 - accuracy: 0.9041 - val_loss: 0.0104 - val_accuracy: 0.8993\n",
            "Epoch 96/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0096 - accuracy: 0.9034 - val_loss: 0.0100 - val_accuracy: 0.9064\n",
            "Epoch 97/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0096 - accuracy: 0.9034 - val_loss: 0.0088 - val_accuracy: 0.9109\n",
            "Epoch 98/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0093 - accuracy: 0.9058 - val_loss: 0.0090 - val_accuracy: 0.9060\n",
            "Epoch 99/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0095 - accuracy: 0.9041 - val_loss: 0.0090 - val_accuracy: 0.9102\n",
            "Epoch 100/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0093 - accuracy: 0.9052 - val_loss: 0.0087 - val_accuracy: 0.9126\n",
            "Epoch 101/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0091 - accuracy: 0.9072 - val_loss: 0.0089 - val_accuracy: 0.9086\n",
            "Epoch 102/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0091 - accuracy: 0.9053 - val_loss: 0.0092 - val_accuracy: 0.9113\n",
            "Epoch 103/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0092 - accuracy: 0.9059 - val_loss: 0.0101 - val_accuracy: 0.9025\n",
            "Epoch 104/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0089 - accuracy: 0.9079 - val_loss: 0.0111 - val_accuracy: 0.8979\n",
            "Epoch 105/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0091 - accuracy: 0.9064 - val_loss: 0.0085 - val_accuracy: 0.9146\n",
            "Epoch 106/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0089 - accuracy: 0.9079 - val_loss: 0.0101 - val_accuracy: 0.8980\n",
            "Epoch 107/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0087 - accuracy: 0.9102 - val_loss: 0.0084 - val_accuracy: 0.9101\n",
            "Epoch 108/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0087 - accuracy: 0.9094 - val_loss: 0.0099 - val_accuracy: 0.8969\n",
            "Epoch 109/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0087 - accuracy: 0.9104 - val_loss: 0.0085 - val_accuracy: 0.9161\n",
            "Epoch 110/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0085 - accuracy: 0.9094 - val_loss: 0.0101 - val_accuracy: 0.9019\n",
            "Epoch 111/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0086 - accuracy: 0.9103 - val_loss: 0.0093 - val_accuracy: 0.9121\n",
            "Epoch 112/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0084 - accuracy: 0.9117 - val_loss: 0.0090 - val_accuracy: 0.9160\n",
            "Epoch 113/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0085 - accuracy: 0.9106 - val_loss: 0.0092 - val_accuracy: 0.9089\n",
            "Epoch 114/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0082 - accuracy: 0.9127 - val_loss: 0.0080 - val_accuracy: 0.9176\n",
            "Epoch 115/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0083 - accuracy: 0.9122 - val_loss: 0.0094 - val_accuracy: 0.9104\n",
            "Epoch 116/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0084 - accuracy: 0.9117 - val_loss: 0.0080 - val_accuracy: 0.9145\n",
            "Epoch 117/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0084 - accuracy: 0.9115 - val_loss: 0.0080 - val_accuracy: 0.9115\n",
            "Epoch 118/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0081 - accuracy: 0.9140 - val_loss: 0.0089 - val_accuracy: 0.9105\n",
            "Epoch 119/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0082 - accuracy: 0.9127 - val_loss: 0.0092 - val_accuracy: 0.9110\n",
            "Epoch 120/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0080 - accuracy: 0.9133 - val_loss: 0.0084 - val_accuracy: 0.9174\n",
            "Epoch 121/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0080 - accuracy: 0.9137 - val_loss: 0.0083 - val_accuracy: 0.9187\n",
            "Epoch 122/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0080 - accuracy: 0.9141 - val_loss: 0.0078 - val_accuracy: 0.9186\n",
            "Epoch 123/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0080 - accuracy: 0.9137 - val_loss: 0.0075 - val_accuracy: 0.9161\n",
            "Epoch 124/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0077 - accuracy: 0.9154 - val_loss: 0.0076 - val_accuracy: 0.9223\n",
            "Epoch 125/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0077 - accuracy: 0.9155 - val_loss: 0.0092 - val_accuracy: 0.9112\n",
            "Epoch 126/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0079 - accuracy: 0.9153 - val_loss: 0.0085 - val_accuracy: 0.9142\n",
            "Epoch 127/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0077 - accuracy: 0.9156 - val_loss: 0.0085 - val_accuracy: 0.9121\n",
            "Epoch 128/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0077 - accuracy: 0.9157 - val_loss: 0.0073 - val_accuracy: 0.9196\n",
            "Epoch 129/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0077 - accuracy: 0.9163 - val_loss: 0.0080 - val_accuracy: 0.9167\n",
            "Epoch 130/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0077 - accuracy: 0.9168 - val_loss: 0.0088 - val_accuracy: 0.9128\n",
            "Epoch 131/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0074 - accuracy: 0.9183 - val_loss: 0.0073 - val_accuracy: 0.9217\n",
            "Epoch 132/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0075 - accuracy: 0.9175 - val_loss: 0.0072 - val_accuracy: 0.9197\n",
            "Epoch 133/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0075 - accuracy: 0.9179 - val_loss: 0.0077 - val_accuracy: 0.9165\n",
            "Epoch 134/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0076 - accuracy: 0.9169 - val_loss: 0.0096 - val_accuracy: 0.9061\n",
            "Epoch 135/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0074 - accuracy: 0.9176 - val_loss: 0.0077 - val_accuracy: 0.9161\n",
            "Epoch 136/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0074 - accuracy: 0.9188 - val_loss: 0.0078 - val_accuracy: 0.9178\n",
            "Epoch 137/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0072 - accuracy: 0.9190 - val_loss: 0.0083 - val_accuracy: 0.9046\n",
            "Epoch 138/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0073 - accuracy: 0.9190 - val_loss: 0.0075 - val_accuracy: 0.9138\n",
            "Epoch 139/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0073 - accuracy: 0.9190 - val_loss: 0.0069 - val_accuracy: 0.9213\n",
            "Epoch 140/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0072 - accuracy: 0.9197 - val_loss: 0.0081 - val_accuracy: 0.9111\n",
            "Epoch 141/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0072 - accuracy: 0.9199 - val_loss: 0.0071 - val_accuracy: 0.9213\n",
            "Epoch 142/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0070 - accuracy: 0.9215 - val_loss: 0.0074 - val_accuracy: 0.9208\n",
            "Epoch 143/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0071 - accuracy: 0.9207 - val_loss: 0.0071 - val_accuracy: 0.9254\n",
            "Epoch 144/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0071 - accuracy: 0.9211 - val_loss: 0.0089 - val_accuracy: 0.9102\n",
            "Epoch 145/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0071 - accuracy: 0.9209 - val_loss: 0.0066 - val_accuracy: 0.9262\n",
            "Epoch 146/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0069 - accuracy: 0.9218 - val_loss: 0.0072 - val_accuracy: 0.9209\n",
            "Epoch 147/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0072 - accuracy: 0.9197 - val_loss: 0.0068 - val_accuracy: 0.9242\n",
            "Epoch 148/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0070 - accuracy: 0.9216 - val_loss: 0.0070 - val_accuracy: 0.9234\n",
            "Epoch 149/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0069 - accuracy: 0.9230 - val_loss: 0.0067 - val_accuracy: 0.9253\n",
            "Epoch 150/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0069 - accuracy: 0.9224 - val_loss: 0.0069 - val_accuracy: 0.9205\n",
            "Epoch 151/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0068 - accuracy: 0.9222 - val_loss: 0.0071 - val_accuracy: 0.9236\n",
            "Epoch 152/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0069 - accuracy: 0.9234 - val_loss: 0.0070 - val_accuracy: 0.9253\n",
            "Epoch 153/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0067 - accuracy: 0.9240 - val_loss: 0.0070 - val_accuracy: 0.9220\n",
            "Epoch 154/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0066 - accuracy: 0.9241 - val_loss: 0.0076 - val_accuracy: 0.9121\n",
            "Epoch 155/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0068 - accuracy: 0.9232 - val_loss: 0.0065 - val_accuracy: 0.9308\n",
            "Epoch 156/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0066 - accuracy: 0.9243 - val_loss: 0.0068 - val_accuracy: 0.9252\n",
            "Epoch 157/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0067 - accuracy: 0.9235 - val_loss: 0.0086 - val_accuracy: 0.9038\n",
            "Epoch 158/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0066 - accuracy: 0.9234 - val_loss: 0.0079 - val_accuracy: 0.9153\n",
            "Epoch 159/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0066 - accuracy: 0.9241 - val_loss: 0.0069 - val_accuracy: 0.9259\n",
            "Epoch 160/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0067 - accuracy: 0.9239 - val_loss: 0.0064 - val_accuracy: 0.9300\n",
            "Epoch 161/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0066 - accuracy: 0.9245 - val_loss: 0.0061 - val_accuracy: 0.9298\n",
            "Epoch 162/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0066 - accuracy: 0.9241 - val_loss: 0.0068 - val_accuracy: 0.9232\n",
            "Epoch 163/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0065 - accuracy: 0.9256 - val_loss: 0.0063 - val_accuracy: 0.9300\n",
            "Epoch 164/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0064 - accuracy: 0.9258 - val_loss: 0.0063 - val_accuracy: 0.9294\n",
            "Epoch 165/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0064 - accuracy: 0.9251 - val_loss: 0.0064 - val_accuracy: 0.9296\n",
            "Epoch 166/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0064 - accuracy: 0.9252 - val_loss: 0.0070 - val_accuracy: 0.9235\n",
            "Epoch 167/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0063 - accuracy: 0.9268 - val_loss: 0.0065 - val_accuracy: 0.9233\n",
            "Epoch 168/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0063 - accuracy: 0.9259 - val_loss: 0.0070 - val_accuracy: 0.9249\n",
            "Epoch 169/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0064 - accuracy: 0.9257 - val_loss: 0.0065 - val_accuracy: 0.9251\n",
            "Epoch 170/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0064 - accuracy: 0.9259 - val_loss: 0.0082 - val_accuracy: 0.9117\n",
            "Epoch 171/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0062 - accuracy: 0.9271 - val_loss: 0.0063 - val_accuracy: 0.9259\n",
            "Epoch 172/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0062 - accuracy: 0.9264 - val_loss: 0.0063 - val_accuracy: 0.9257\n",
            "Epoch 173/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0062 - accuracy: 0.9269 - val_loss: 0.0063 - val_accuracy: 0.9278\n",
            "Epoch 174/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0062 - accuracy: 0.9269 - val_loss: 0.0065 - val_accuracy: 0.9277\n",
            "Epoch 175/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0062 - accuracy: 0.9278 - val_loss: 0.0062 - val_accuracy: 0.9298\n",
            "Epoch 176/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0062 - accuracy: 0.9272 - val_loss: 0.0071 - val_accuracy: 0.9268\n",
            "Epoch 177/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0062 - accuracy: 0.9273 - val_loss: 0.0060 - val_accuracy: 0.9330\n",
            "Epoch 178/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0061 - accuracy: 0.9272 - val_loss: 0.0060 - val_accuracy: 0.9305\n",
            "Epoch 179/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0061 - accuracy: 0.9282 - val_loss: 0.0060 - val_accuracy: 0.9324\n",
            "Epoch 180/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0060 - accuracy: 0.9286 - val_loss: 0.0059 - val_accuracy: 0.9328\n",
            "Epoch 181/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0060 - accuracy: 0.9296 - val_loss: 0.0060 - val_accuracy: 0.9334\n",
            "Epoch 182/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0060 - accuracy: 0.9291 - val_loss: 0.0065 - val_accuracy: 0.9301\n",
            "Epoch 183/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0061 - accuracy: 0.9281 - val_loss: 0.0058 - val_accuracy: 0.9332\n",
            "Epoch 184/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0060 - accuracy: 0.9285 - val_loss: 0.0058 - val_accuracy: 0.9351\n",
            "Epoch 185/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0059 - accuracy: 0.9293 - val_loss: 0.0058 - val_accuracy: 0.9333\n",
            "Epoch 186/250\n",
            "423/423 [==============================] - 3s 7ms/step - loss: 0.0060 - accuracy: 0.9285 - val_loss: 0.0059 - val_accuracy: 0.9331\n",
            "Epoch 187/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0059 - accuracy: 0.9305 - val_loss: 0.0060 - val_accuracy: 0.9287\n",
            "Epoch 188/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0059 - accuracy: 0.9292 - val_loss: 0.0058 - val_accuracy: 0.9306\n",
            "Epoch 189/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0058 - accuracy: 0.9296 - val_loss: 0.0061 - val_accuracy: 0.9291\n",
            "Epoch 190/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0057 - accuracy: 0.9306 - val_loss: 0.0058 - val_accuracy: 0.9325\n",
            "Epoch 191/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0058 - accuracy: 0.9300 - val_loss: 0.0058 - val_accuracy: 0.9349\n",
            "Epoch 192/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0058 - accuracy: 0.9295 - val_loss: 0.0061 - val_accuracy: 0.9311\n",
            "Epoch 193/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0057 - accuracy: 0.9309 - val_loss: 0.0057 - val_accuracy: 0.9354\n",
            "Epoch 194/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0058 - accuracy: 0.9298 - val_loss: 0.0059 - val_accuracy: 0.9362\n",
            "Epoch 195/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0057 - accuracy: 0.9305 - val_loss: 0.0058 - val_accuracy: 0.9348\n",
            "Epoch 196/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0057 - accuracy: 0.9313 - val_loss: 0.0062 - val_accuracy: 0.9296\n",
            "Epoch 197/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0057 - accuracy: 0.9315 - val_loss: 0.0061 - val_accuracy: 0.9320\n",
            "Epoch 198/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0058 - accuracy: 0.9312 - val_loss: 0.0057 - val_accuracy: 0.9326\n",
            "Epoch 199/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0057 - accuracy: 0.9317 - val_loss: 0.0058 - val_accuracy: 0.9281\n",
            "Epoch 200/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0056 - accuracy: 0.9307 - val_loss: 0.0058 - val_accuracy: 0.9317\n",
            "Epoch 201/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0057 - accuracy: 0.9312 - val_loss: 0.0056 - val_accuracy: 0.9367\n",
            "Epoch 202/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0056 - accuracy: 0.9315 - val_loss: 0.0057 - val_accuracy: 0.9297\n",
            "Epoch 203/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0056 - accuracy: 0.9306 - val_loss: 0.0061 - val_accuracy: 0.9288\n",
            "Epoch 204/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0056 - accuracy: 0.9320 - val_loss: 0.0091 - val_accuracy: 0.9102\n",
            "Epoch 205/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0055 - accuracy: 0.9326 - val_loss: 0.0063 - val_accuracy: 0.9269\n",
            "Epoch 206/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0055 - accuracy: 0.9316 - val_loss: 0.0055 - val_accuracy: 0.9357\n",
            "Epoch 207/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0055 - accuracy: 0.9323 - val_loss: 0.0057 - val_accuracy: 0.9357\n",
            "Epoch 208/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0054 - accuracy: 0.9327 - val_loss: 0.0055 - val_accuracy: 0.9362\n",
            "Epoch 209/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0056 - accuracy: 0.9318 - val_loss: 0.0058 - val_accuracy: 0.9334\n",
            "Epoch 210/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0054 - accuracy: 0.9316 - val_loss: 0.0064 - val_accuracy: 0.9308\n",
            "Epoch 211/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0055 - accuracy: 0.9323 - val_loss: 0.0058 - val_accuracy: 0.9321\n",
            "Epoch 212/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0054 - accuracy: 0.9331 - val_loss: 0.0053 - val_accuracy: 0.9353\n",
            "Epoch 213/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0053 - accuracy: 0.9325 - val_loss: 0.0059 - val_accuracy: 0.9336\n",
            "Epoch 214/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0054 - accuracy: 0.9333 - val_loss: 0.0064 - val_accuracy: 0.9305\n",
            "Epoch 215/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0053 - accuracy: 0.9336 - val_loss: 0.0058 - val_accuracy: 0.9326\n",
            "Epoch 216/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0054 - accuracy: 0.9324 - val_loss: 0.0062 - val_accuracy: 0.9269\n",
            "Epoch 217/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0054 - accuracy: 0.9335 - val_loss: 0.0056 - val_accuracy: 0.9358\n",
            "Epoch 218/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0053 - accuracy: 0.9343 - val_loss: 0.0071 - val_accuracy: 0.9269\n",
            "Epoch 219/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0053 - accuracy: 0.9347 - val_loss: 0.0056 - val_accuracy: 0.9370\n",
            "Epoch 220/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0052 - accuracy: 0.9343 - val_loss: 0.0051 - val_accuracy: 0.9382\n",
            "Epoch 221/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0053 - accuracy: 0.9334 - val_loss: 0.0057 - val_accuracy: 0.9339\n",
            "Epoch 222/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0053 - accuracy: 0.9341 - val_loss: 0.0053 - val_accuracy: 0.9378\n",
            "Epoch 223/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0053 - accuracy: 0.9345 - val_loss: 0.0051 - val_accuracy: 0.9414\n",
            "Epoch 224/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0052 - accuracy: 0.9347 - val_loss: 0.0072 - val_accuracy: 0.9268\n",
            "Epoch 225/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0052 - accuracy: 0.9342 - val_loss: 0.0055 - val_accuracy: 0.9309\n",
            "Epoch 226/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0052 - accuracy: 0.9351 - val_loss: 0.0050 - val_accuracy: 0.9405\n",
            "Epoch 227/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0051 - accuracy: 0.9354 - val_loss: 0.0068 - val_accuracy: 0.9297\n",
            "Epoch 228/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0051 - accuracy: 0.9349 - val_loss: 0.0069 - val_accuracy: 0.9186\n",
            "Epoch 229/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0051 - accuracy: 0.9358 - val_loss: 0.0053 - val_accuracy: 0.9354\n",
            "Epoch 230/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0052 - accuracy: 0.9349 - val_loss: 0.0054 - val_accuracy: 0.9385\n",
            "Epoch 231/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0051 - accuracy: 0.9360 - val_loss: 0.0054 - val_accuracy: 0.9400\n",
            "Epoch 232/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0051 - accuracy: 0.9353 - val_loss: 0.0052 - val_accuracy: 0.9389\n",
            "Epoch 233/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0051 - accuracy: 0.9354 - val_loss: 0.0050 - val_accuracy: 0.9417\n",
            "Epoch 234/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0051 - accuracy: 0.9348 - val_loss: 0.0054 - val_accuracy: 0.9370\n",
            "Epoch 235/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0050 - accuracy: 0.9354 - val_loss: 0.0063 - val_accuracy: 0.9236\n",
            "Epoch 236/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0050 - accuracy: 0.9358 - val_loss: 0.0052 - val_accuracy: 0.9353\n",
            "Epoch 237/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0051 - accuracy: 0.9350 - val_loss: 0.0049 - val_accuracy: 0.9419\n",
            "Epoch 238/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0050 - accuracy: 0.9360 - val_loss: 0.0052 - val_accuracy: 0.9340\n",
            "Epoch 239/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0051 - accuracy: 0.9355 - val_loss: 0.0048 - val_accuracy: 0.9407\n",
            "Epoch 240/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0049 - accuracy: 0.9366 - val_loss: 0.0059 - val_accuracy: 0.9377\n",
            "Epoch 241/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0050 - accuracy: 0.9362 - val_loss: 0.0057 - val_accuracy: 0.9271\n",
            "Epoch 242/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0049 - accuracy: 0.9368 - val_loss: 0.0049 - val_accuracy: 0.9399\n",
            "Epoch 243/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0050 - accuracy: 0.9360 - val_loss: 0.0049 - val_accuracy: 0.9382\n",
            "Epoch 244/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0049 - accuracy: 0.9371 - val_loss: 0.0052 - val_accuracy: 0.9401\n",
            "Epoch 245/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0049 - accuracy: 0.9366 - val_loss: 0.0053 - val_accuracy: 0.9372\n",
            "Epoch 246/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0049 - accuracy: 0.9375 - val_loss: 0.0047 - val_accuracy: 0.9439\n",
            "Epoch 247/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0048 - accuracy: 0.9372 - val_loss: 0.0064 - val_accuracy: 0.9266\n",
            "Epoch 248/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0049 - accuracy: 0.9365 - val_loss: 0.0052 - val_accuracy: 0.9378\n",
            "Epoch 249/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0048 - accuracy: 0.9374 - val_loss: 0.0050 - val_accuracy: 0.9412\n",
            "Epoch 250/250\n",
            "423/423 [==============================] - 3s 8ms/step - loss: 0.0049 - accuracy: 0.9365 - val_loss: 0.0055 - val_accuracy: 0.9355\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/ml-output/escher/model/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifbsQ-itw6Ut"
      },
      "source": [
        "jsfunc=\"\"\"\n",
        "function ClickConnect(){\n",
        "  colab.config\n",
        "  console.log(\"Connnect Clicked - Start\"); \n",
        "  document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click();\n",
        "  console.log(\"Connnect Clicked - End\");\n",
        "};\n",
        "setInterval(ClickConnect, 1000000)\n",
        "\"\"\""
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2AaGdhexWkK"
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": []
    }
  ]
}